{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Data Lake Storage Connector - Quick Start\n",
    "\n",
    "The AzureBob storage connector enables you to read/write data within the Azure Blob Storage and Azure Data Lake with ease and integrate it with YData's platform.\n",
    "Reading a dataset from AzureBlob's directly into a YData's `Dataset` allows its usage for Data Quality, Data Synthetisation and Preprocessing blocks.\n",
    "\n",
    "The AzureBlobConnector allows the user to perform the following actions:\n",
    "- **AzureBlobConnector.get_table** - Reads the data available within a given schema from a selected database. Returns a Dataset object.\n",
    "- **AzureBlobConnector.query** - Reads the data retrieved by a database query. Returns a Dataset object.\n",
    "- **AzureBlobConnector.sample_query** - Reads a sample (sample_size) from the a query retrieved data. Returns a Dataset object.\n",
    "- **AzureBlobConnector.read_database** - Reads the full database data. \n",
    "- **AzureBlobConnector.write_table** - Writes a Dataset into a new schema table.\n",
    "\n",
    "This tutorial covers:\n",
    "- How to read data from Azure Data Lake Storage\n",
    "- How to read data (sample) from Azure Data Lake Storage\n",
    "- How to write data to Azure Data Lake Storage\n",
    "- Read data from Synapses Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata.connectors import AzureBlobConnector\n",
    "from ydata.connectors.filetype import FileType\n",
    "from ydata.utils.formats import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Connector\n",
    "connector =  AzureBlobConnector(account_name='ydatasynapse',\n",
    "                                account_key='e/lAyP1M76I0ZaT3LTAWDf5hQqg7YBOxvmeVfBxzRhAKw+3E8gDRQJlDukzHW1q+X4oAlaVGhweH+ASt7Stssw==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "# The file_type argument is optional. If not provided, we will infer it from the path you have provided.\n",
    "#Add here the detail on the file path conversation for the Azure Data Lake\n",
    "\n",
    "#https://{insert-host-name}.dfs.core.windows.net/{insert-container-name}/paysim.csv\n",
    "# We need to have the following format instead\n",
    "##abfss://{insert-container-name}@{insert-host-name}.dfs.core.windows.net/{insert-file-path}\n",
    "\n",
    "data = connector.read_file('abfss://{insert-container-name}@{insert-host-name}.dfs.core.windows.net/{insert-file-path}.csv', file_type=FileType.CSV)\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a quick glimpse, we can load a small subset of the data - let's say 100 rows\n",
    "very_small_data = connector.read_sample('abfss://{insert-container-name}@{insert-host-name}.dfs.core.windows.net/{insert-file-path}.csv', sample_size=100, file_type=FileType.CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now imagine we want to store the sampled data.\n",
    "connector.write_file(very_small_data, 'abfss://{insert-container-name}@{insert-host-name}.dfs.core.windows.net/{insert-file-path}.csv', file_type=FileType.CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a specific blob\n",
    "connector.delete_blob_if_exists('abfs://{insert-blob-name}/{insert-file-path}.csv', file_type=FileType.CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading multiple CSV files from a folder\n",
    "data = connector.read('abfss://{insert-container-name}@{insert-host-name}.dfs.core.windows.net/{insert-file-path}.csv', file_type=FileType.CSV)\n",
    "\n",
    "#orfss\n",
    "\n",
    "data = connector.read('abfss://{insert-container-name}@{insert-host-name}.dfs.core.windows.net/{insert-file-path}.csv', file_type=FileType.CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple - Reading data from Synapse ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading from Synapse Data Lake\n",
    "connector =  AzureBlobConnector(account_name='{insert-synapse-data-lake-accountname}',\n",
    "                                account_key='{insert-synapse-data-lake-accountkey}')\n",
    "\n",
    "#File path from ADLS:\n",
    "#https://{insert-resource-name}.blob.core.windows.net/{insert-container-name}/{insert-file-path}\n",
    "#we need to convert it to\n",
    "#abfs://{insert-container-name}@ydatasynapse.dfs.core.windows.net/{insert-file-path}\n",
    "\n",
    "data = connector.read_file('abfs://{insert-container-name}@{insert-host-name}.dfs.core.windows.net/{insert-file-path}.csv', file_type=FileType.CSV)\n",
    "\n",
    "#https://ydata-test@ydatasynapse.dfs.core.windows.net/paysim.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
