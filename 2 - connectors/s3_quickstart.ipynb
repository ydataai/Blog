{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3Connector - Quick Start\n",
    "\n",
    "The S3Connector enables you to read/write data within the AWS Simple Storage Service with ease and integrate it with YData's platform.\n",
    "Reading a dataset from S3 directly into a YData's `Dataset` allows its usage for Data Quality, Data Synthetisation and Preprocessing blocks.\n",
    "\n",
    "The GCSConnector allows the user to perform the following actions:\n",
    "- **S3Connector.check_bucket** - Validates wether a given bucket is available.\n",
    "- **S3Connector.ls** - List the files available at a give bucket path.\n",
    "- **S3Connector.read** - Reads from a folder or file/blob given a certain file_type (Parquet or CSV).\n",
    "- **S3Connector.read_sample** - Reads a sample (sample_size) from a folder or file/blob given a certain file_type (Parquet or CSV).\n",
    "\n",
    "This tutorial covers:\n",
    "- How to read data from S3\n",
    "- How to read data (sample) from S3\n",
    "- How to write data to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from ydata.connectors import S3Connector\n",
    "from ydata.connectors.filetype import FileType\n",
    "from ydata.utils.formats import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your credentials from a file\n",
    "#token = read_json('{insert-path-to-credentials}')\n",
    "\n",
    "token = read_json('s3_credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/distributed/client.py:1288: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | client | scheduler | workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| toolz   | 0.11.2 | 0.12.0    | None    |\n",
      "+---------+--------+-----------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Connector\n",
    "connector = S3Connector(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "data = connector.read_file('s3://{insert-bucket}/{insert-filepath}', file_type=FileType.CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter \"sep\" and \"has_header\" is only considered for fFileType.CSV files.\n"
     ]
    }
   ],
   "source": [
    "# The file_type argument is optional. If not provided, we will infer it from the path you have provided.\n",
    "parquet_data = connector.read_file('S3://{insert-bucket}/{insert-filepath}.parquet', file_type=FileType.PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a quick glimpse, we can load a small subset of the data - let's say 100 rows\n",
    "small_data = connector.read_sample('s3://{insert-bucket}/{insert-filepath}', sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now imagine we want to store the sampled data.\n",
    "connector.write_file(data, 's3://{insert-bucket}/{insert-filepath}', file_type=FileType.CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can write a new Dataframe \n",
    "from pandas.util.testing import makeDataFrame\n",
    "dummy_df = makeDataFrame()\n",
    "connector.write_file(dummy_df, 's3://{insert-bucket}/{insert-filepath}', write_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "Connectors provided developer utilities that enable Data Scientists to navigate S3 Storage via code blocks.\n",
    "\n",
    "* Check if a bucket exists\n",
    "* List the contents of a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check if a certain bucket exists\n",
    "connector.check_bucket('{insert-bucket}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the contents of a certain bucket\n",
    "connector.list(bucket_name='{insert-bucket}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the contents of the prefix\n",
    "connector.list('{insert-bucket}', prefix='{insert-prefix}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
