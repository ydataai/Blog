{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0eb9bfb6d2367f5ad13c071e9ca0a4847a6ac5b11a61adc2b39beabe858de7ad1",
   "display_name": "Python 3.7.9 64-bit ('ydata_tmp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# S3Connector - Quick Start\n",
    "\n",
    "The S3Connector enables you to read/write data within the AWS Simple Storage Service with ease and integrate it with YData's platform.\n",
    "Reading a dataset from S3 directly into a YData's `Dataset` allows its usage for Data Quality, Data Synthetisation and Preprocessing blocks.\n",
    "\n",
    "The following tutorial covers:\n",
    "- How to read data from S3\n",
    "- How to read data (sample) from S3\n",
    "- How to write data to S3\n",
    "- (Advanced) Developer utilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from ydata.connectors import S3Connector\n",
    "from ydata.connectors.filetype import FileType\n",
    "from ydata.utils.formats import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your credentials from a file\n",
    "token = read_json('{insert-path-to-credentials}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Connector\n",
    "connector = S3Connector(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "data = connector.read_file('s3://{insert-bucket}/{insert-filepath}', file_type=FileType.CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file_type argument is optional. If not provided, we will infer it from the path you have provided.\n",
    "parquet_data = connector.read_file('S3://{insert-bucket}/{insert-filepath}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a quick glimpse, we can load a small subset of the data - let's say 100 rows\n",
    "small_data = connector.read_sample('s3://{insert-bucket}/{insert-filepath}', sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now imagine we want to store the sampled data.\n",
    "connector.write_file(data, 's3://{insert-bucket}/{insert-filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can write a new Dataframe \n",
    "from pandas.util.testing import makeDataFrame\n",
    "dummy_df = makeDataFrame()\n",
    "connector.write_file(dummy_df, 's3://{insert-bucket}/{insert-filepath}', write_index=True)"
   ]
  },
  {
   "source": [
    "## Advanced Features\n",
    "Connectors provided developer utilities that enable Data Scientists to navigate S3 Storage via code blocks.\n",
    "\n",
    "* Check if a bucket exists\n",
    "* List the contents of a bucket"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check if a certain bucket exists\n",
    "connector.check_bucket('{insert-bucket}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the contents of a certain bucket\n",
    "connector.list(bucket_name='{insert-bucket}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the contents of the prefix\n",
    "connector.list('{insert-bucket}', prefix='{insert-prefix}')"
   ]
  }
 ]
}