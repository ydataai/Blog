{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Imputation\n",
    "This notebook loads the preprocessed data and impute the missing values for each station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 - Imports\n",
    "Load the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dask_gateway/client.py:21: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import LoopRunner, format_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/tslearn/clustering/kmeans.py:17: UserWarning: Scikit-learn <0.24 will be deprecated in a future release of tslearn\n",
      "  \"Scikit-learn <0.24 will be deprecated in a \"\n"
     ]
    }
   ],
   "source": [
    "from ydata.connectors import LocalConnector, GCSConnector\n",
    "from ydata.utils.formats import read_json\n",
    "from ydata.quality.impute.timeseries import TSMissingImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 - Auxiliary Functions\n",
    "The auxiliary functions are custom-designed utilities developed for the REE use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata.dataset import Dataset\n",
    "\n",
    "from utils import setting_index_data\n",
    "from imputation import get_cold_start_meters, resample_station_data, data_boundaries, load_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Data\n",
    "Train data comprises the preprocessed readings until August 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connector for Google Cloud Storage\n",
    "connector = LocalConnector()\n",
    "\n",
    "# Read the train data\n",
    "data = connector.read_file('train_allmeters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the factors\n",
    "add_factors = load_factors('df_factors_2018_2021.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Data Wrangling\n",
    "Parse the data into the correct types and with the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data to be ready for imputation\n",
    "data = setting_index_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Cold Start\n",
    "Training on cold-start meters (i.e. without any observed values) should be made in separate from the rest of the meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aysha1', 'gode1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of cold-start meters\n",
    "cold_start_meters = get_cold_start_meters(data)\n",
    "cold_start_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the cold-start meters from the train data.\n",
    "train_data = data[~data['station'].isin(cold_start_meters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Data Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_boundaries(train_data, replace_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station           0\n",
       "speed        470005\n",
       "direction    468570\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Imputer\n",
    "The TSMissingImputer is responsible to impute the missing values for time-series.\n",
    "- Learns the temporal dynamics from the observed values\n",
    "- Supports multiple entities with the `partition_by` parameter\n",
    "- Follows the usual scikit-learn method interfaces (e.g. fit, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Train the TSMissing Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Imputer\n",
    "imputer = TSMissingImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSMissingImputer()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Imputer\n",
    "imputer.fit(train_data, partition_by='station', num_cols=['speed'], add_factors=add_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Impute for Full Year\n",
    "Construct a full year of data, on hourly basis, for devices with observed readings. For each hour, the average of windspeed/winddirection is calculated and used as ground-truth for observed readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of a whole year for all the meters with observed values.\n",
    "whole_year = resample_station_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the missing values imputation to reconstruct a whole year of data.\n",
    "reconstructed = imputer.transform(whole_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Impute for Holdout\n",
    "Construct a full month of holdout, on hourly basis, for devices with observed readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the missing values imputation to reconstruct the holdout period\n",
    "holdout = connector.read_file('holdout_allmeters.csv')\n",
    "holdout = preprocess_data(holdout)\n",
    "\n",
    "# Remove the cold-start meters from holdout data.\n",
    "holdout = holdout[~holdout['station'].isin(cold_start_meters)]\n",
    "whole_holdout = resample_station_data(holdout, start_ts='2021-03-01', end_ts='2021-04-30')\n",
    "holdout_reconstructed = imputer.transform(whole_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After reconstruction, no value should be missing\n",
    "assert reconstructed.isna().sum().sum() == 0, \"The reconstructed dataset contains missing values after reconstruction.\"\n",
    "assert holdout_reconstructed.isna().sum().sum() == 0, \"The reconstructed dataset of holdout contains missing values after reconstruction.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Data PostProcessing\n",
    "The imputation of time-series is applicable to any type of numerical data and thus agnostic to energy-specific boundaries of wind measurements. To guarantee adequacy for wind speed and direction, we enforce that wind speed cannot be negative and that wind direction should range within degree angles (between 0 and 360)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocess the training data\n",
    "postprocessed = data_boundaries(data=reconstructed)\n",
    "\n",
    "# Postprocess the holdout data\n",
    "postprocessed_holdout = data_boundaries(data=holdout_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Store Data\n",
    "After the data is fully reconstructed, store to cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: 2022-02-18 14:44:47,882 Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1391, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "OSError: [Errno 0] Error\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 696, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1478, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1429, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 611, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 641, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 189, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 732, in _handle_events\n",
      "    self.close(exc_info=e)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 611, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 641, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "ERROR: 2022-02-18 14:44:47,885 Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1391, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "OSError: [Errno 0] Error\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 696, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1478, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 1429, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 611, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 641, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 189, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 732, in _handle_events\n",
      "    self.close(exc_info=e)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 611, in close\n",
      "    self._signal_closed()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/iostream.py\", line 641, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# Store the whole year reconstructed\n",
    "connector.write_file(data=postprocessed.reset_index(), path='whole_year_reconstructed.csv', index=True)\n",
    "\n",
    "# Store the holdout\n",
    "connector.write_file(data=postprocessed_holdout.reset_index(), path='holdout_reconstructed.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ea8bb7365730d103fd67461bc9b3ce908dae9a09e6c17b8942395d50e6cfca7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
